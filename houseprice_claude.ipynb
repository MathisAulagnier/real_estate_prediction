{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Model Selection\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import (\n",
    "    OrdinalEncoder, \n",
    "    StandardScaler, \n",
    "    OneHotEncoder, \n",
    "    MinMaxScaler\n",
    ")\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import (\n",
    "    LinearRegression, \n",
    "    Ridge\n",
    ")\n",
    "\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestRegressor, \n",
    "    GradientBoostingRegressor, \n",
    "    VotingRegressor, \n",
    "    StackingRegressor\n",
    ")\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Model Selection\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('home-data-for-ml-course/train.csv')\n",
    "test_df = pd.read_csv('home-data-for-ml-course/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numerical columns only\n",
    "X_num = train_df.select_dtypes(include=[np.number]).columns.to_list()\n",
    "\n",
    "# Select categorical columns only\n",
    "X_cat = train_df.select_dtypes(include=[object]).columns.to_list()\n",
    "\n",
    "print(X_num)\n",
    "print(X_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normaliser les données\n",
    "train_df_numeric = train_df.select_dtypes(include=[int, float])\n",
    "\n",
    "# Créer la pipeline pour imputer les valeurs manquantes et normaliser les données\n",
    "pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")),  # Remplacer les NaN par la moyenne\n",
    "    (\"scaler\", MinMaxScaler())                    # Normaliser les données entre 0 et 1\n",
    "])\n",
    "\n",
    "# Appliquer la pipeline aux données\n",
    "data_processed = pipeline.fit_transform(train_df_numeric)\n",
    "\n",
    "# Si tu veux convertir le résultat en DataFrame\n",
    "data_processed_df = pd.DataFrame(data_processed, columns=train_df_numeric.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir l'architecture de l'auto-encodeur\n",
    "input_dim = data_processed.shape[1] \n",
    "print(f'nombre de variables : {input_dim}')\n",
    "\n",
    "autoencoder = Sequential([\n",
    "    Dense(32, activation=\"tanh\", input_shape=(input_dim,)),\n",
    "    Dense(16, activation=\"tanh\"),\n",
    "    Dense(12, activation=\"tanh\"),  # Couche de compression\n",
    "    Dense(16, activation=\"tanh\"),\n",
    "    Dense(32, activation=\"tanh\"),\n",
    "    Dense(input_dim, activation=\"sigmoid\")  # La sortie doit être de la même dimension que l'entrée\n",
    "])\n",
    "\n",
    "# Compiler le modèle\n",
    "autoencoder.compile(optimizer=\"adam\", loss='mse')\n",
    "\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = autoencoder.fit(data_processed, data_processed, epochs=30, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenir les prédictions\n",
    "predictions = autoencoder.predict(data_processed)\n",
    "\n",
    "# Calculer les erreurs de reconstruction\n",
    "reconstruction_error = np.mean(np.square(data_processed - predictions), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir le seuil comme la moyenne + un multiple de l'écart-type\n",
    "threshold_multiplier = 1\n",
    "threshold = np.mean(reconstruction_error) + threshold_multiplier * np.std(reconstruction_error)\n",
    "threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher les erreurs de reconstruction\n",
    "plt.hist(reconstruction_error, bins=50)\n",
    "plt.xlabel('Reconstruction Error')\n",
    "plt.ylabel('Number of samples')\n",
    "plt.axvline(x=threshold, color='red', linestyle='--')\n",
    "plt.show()\n",
    "\n",
    "print(f'Le nombre de valeurs aberrantes est de {np.sum(reconstruction_error > threshold)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifier les outliers\n",
    "outliers = reconstruction_error > threshold\n",
    "\n",
    "# Recontruire le DataFrame original sans les outliers\n",
    "train_df_no_outliers = train_df[~outliers]\n",
    "len(train_df_no_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = [598, 955, 935, 1299, 250, 314, 336, 707, 379, 1183, 692, 186, 441, 186, 524, 739, 598, 955, 636, 1062, 1191, 496, 198, 1338]\n",
    "train_df_video = train_df[train_df.Id.isin(values) == False]\n",
    "len(train_df_video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intersection des deux dataframes\n",
    "train_df_target = pd.merge(train_df_no_outliers, train_df_video, how='inner')\n",
    "len(train_df_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nous allons regarder les colonnes qui ont des valeurs manquantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(train_df_target.isnull().sum()).sort_values(by=0, ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PoolQC -> Pool Quality\n",
    "train_df_target = train_df_target.drop('PoolQC', axis=1)\n",
    "test_df = test_df.drop('PoolQC', axis=1)\n",
    "# MiscFeature -> Miscellaneous feature not covered in other categories\n",
    "train_df_target = train_df_target.drop('MiscFeature', axis=1)\n",
    "test_df = test_df.drop('MiscFeature', axis=1)\n",
    "# Alley -> Type of alley access to property\n",
    "train_df_target = train_df_target.drop('Alley', axis=1)\n",
    "test_df = test_df.drop('Alley', axis=1)\n",
    "# Fence -> Fence quality\n",
    "train_df_target = train_df_target.drop('Fence', axis=1)\n",
    "test_df = test_df.drop('Fence', axis=1)\n",
    "# MasVnrType -> Masonry veneer type\n",
    "train_df_target['MasVnrType'].fillna('Unknown', inplace=True)\n",
    "sns.catplot(data=train_df_target, x=\"MasVnrType\", y=\"SalePrice\", kind=\"box\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## On remarque que les valeurs 'BrkCmn' et 'Unknown' ont des prix de vente moyen similaires\n",
    "train_df_target['MasVnrType'] = train_df_target['MasVnrType'].replace('Unknown', 'BrkCmn')\n",
    "test_df['MasVnrType'].fillna('BrkCmn', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FireplaceQu -> Fireplace quality\n",
    "train_df_target['FireplaceQu'].fillna('Unknown', inplace=True)\n",
    "sns.catplot(data=train_df, x=\"FireplaceQu\", y=\"SalePrice\", kind=\"box\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_target['FireplaceQu'] = train_df_target['FireplaceQu'].replace('Unknown', 'Po')\n",
    "test_df['FireplaceQu'].fillna('Po', inplace=True)\n",
    "# LotFrontage -> Linear feet of street connected to property (remplacer les NaN par la médiane)\n",
    "train_df_target['LotFrontage'].fillna(train_df_target['LotFrontage'].median(), inplace=True)\n",
    "\n",
    "# GarageYrBlt -> Year garage was built \n",
    "print(f'Correlation between GarageYrBlt and SalePrice: {train_df_target[\"GarageYrBlt\"].corr(train_df_target[\"SalePrice\"])}')\n",
    "train_df_target = train_df_target.drop('GarageYrBlt', axis=1)\n",
    "test_df = test_df.drop('GarageYrBlt', axis=1)\n",
    "\n",
    "# GarageCond -> Garage condition\n",
    "train_df_target = train_df_target.drop(columns='GarageCond')\n",
    "test_df = test_df.drop(columns='GarageCond')\n",
    "\n",
    "# GarageType -> Garage location\n",
    "train_df_target['GarageType'].fillna('Unknown', inplace=True)\n",
    "test_df['GarageType'].fillna('Unknown', inplace=True)\n",
    "\n",
    "# GarageFinish -> Interior finish of the garage\n",
    "train_df_target['GarageFinish'].fillna('Unf', inplace=True)\n",
    "test_df['GarageFinish'].fillna('Unf', inplace=True)\n",
    "\n",
    "# GarageQual -> Garage quality\n",
    "train_df_target['GarageQual'].fillna('TA', inplace=True)\n",
    "test_df['GarageQual'].fillna('TA', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BsmtFinType2 \n",
    "train_df_target = train_df_target.drop('BsmtFinType2', axis=1)\n",
    "test_df = test_df.drop('BsmtFinType2', axis=1)\n",
    "\n",
    "# BsmtExposure  \n",
    "train_df_target['BsmtExposure'].fillna('NoBasement', inplace=True)\n",
    "test_df['BsmtExposure'].fillna('NoBasement', inplace=True)\n",
    "\n",
    "# BsmtQual\n",
    "train_df_target['BsmtQual'].fillna('NoBasement', inplace=True)\n",
    "test_df['BsmtQual'].fillna('NoBasement', inplace=True)\n",
    "\n",
    "# BsmtCond\n",
    "train_df_target['BsmtCond'].fillna('NoBasement', inplace=True)\n",
    "test_df['BsmtCond'].fillna('NoBasement', inplace=True)\n",
    "\n",
    "# BsmtFinType1\n",
    "train_df_target['BsmtFinType1'].fillna('NoBasement', inplace=True)\n",
    "sns.catplot(data=train_df_target, x=\"BsmtFinType1\", y=\"SalePrice\", kind=\"box\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_target['BsmtFinType1'] = train_df_target['BsmtFinType1'].replace('NoBasement', 'Unf')\n",
    "test_df['BsmtFinType1'].fillna('Unf', inplace=True)\n",
    "train_df_target['MasVnrArea'].fillna(0, inplace=True)\n",
    "test_df['MasVnrArea'].fillna(0, inplace=True)\n",
    "train_df_target['Electrical'].fillna('SBrkr', inplace=True)\n",
    "test_df['Electrical'].fillna('SBrkr', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_target = train_df_target.drop(columns=['GarageArea'])\n",
    "test_df = test_df.drop(columns=['GarageArea'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Il a été remarqué qu'appliquer le logarithme sur la variable cible permet de mieux modéliser les données\n",
    "train_df_target['SalePrice'] = np.log1p(train_df_target['SalePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(\n",
    "    train_df_target,\n",
    "    x=train_df_target['SalePrice']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_columns = [\n",
    "    'LotShape', \n",
    "    'LandContour',\n",
    "    'Utilities',\n",
    "    'LandSlope',  \n",
    "    'BsmtQual',  \n",
    "    'BsmtFinType1',  \n",
    "    'CentralAir',  \n",
    "    'Functional', \n",
    "    'FireplaceQu', \n",
    "    'GarageFinish', \n",
    "    'GarageQual', \n",
    "    'PavedDrive', \n",
    "    'ExterCond', \n",
    "    'KitchenQual', \n",
    "    'BsmtExposure', \n",
    "    'HeatingQC',\n",
    "    'ExterQual', \n",
    "    'BsmtCond'\n",
    "]\n",
    "\n",
    "categorical_columns = [\n",
    "    'Street', \n",
    "    'LotConfig',\n",
    "    'Neighborhood', \n",
    "    'Condition1', \n",
    "    'Condition2', \n",
    "    'BldgType', \n",
    "    'HouseStyle', \n",
    "    'RoofStyle', \n",
    "    'Exterior1st', \n",
    "    'Exterior2nd',\n",
    "    'MasVnrType',\n",
    "    'Foundation',  \n",
    "    'Electrical',  \n",
    "    'SaleType', \n",
    "    'MSZoning', \n",
    "    'SaleCondition', \n",
    "    'Heating', \n",
    "    'GarageType', \n",
    "    'RoofMatl'\n",
    "]\n",
    "\n",
    "numerical_columns = train_df_target.select_dtypes(include=[np.number]).columns.to_list()\n",
    "numerical_columns.remove('SalePrice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')), # Remplace les valeurs manquantes par la moyenne\n",
    "    ('scaler', StandardScaler()) # Standardise les valeurs (Moyenne = 0, Ecart-type = 1)\n",
    "]) \n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')), # Remplace les valeurs manquantes par 'missing'\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore')) # Encodage one-hot\n",
    "])\n",
    "\n",
    "ordinal_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')), # Remplace les valeurs manquantes par 'missing'\n",
    "    ('ordinal', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)) # Encodage ordinal\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numerical_columns),\n",
    "        ('cat', categorical_transformer, categorical_columns),\n",
    "        ('ord', ordinal_transformer, ordinal_columns)\n",
    "    ],\n",
    "    remainder='passthrough', # Ignore les colonnes non transformées\n",
    "    n_jobs=-1 # Utilise tous les coeurs du CPU\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df_target.drop(columns='SalePrice')\n",
    "y = train_df_target['SalePrice']\n",
    "\n",
    "X.drop(columns='Id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_preprocessed = pipeline.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_preprocessed, y, test_size=0.2, random_state=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "from sklearn.linear_model import (\n",
    "    LinearRegression, \n",
    "    Ridge\n",
    ")\n",
    "\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestRegressor, \n",
    "    GradientBoostingRegressor, \n",
    "    VotingRegressor, \n",
    "    StackingRegressor\n",
    ")\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "print(f'Linear Regression: {mean_squared_error(y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr = RandomForestRegressor()\n",
    "\n",
    "rfr_param_grid = {\n",
    "    'max_depth': [5, 10, 15],\n",
    "    'n_estimators': [100, 250, 500],\n",
    "    'min_samples_split': [3, 5, 10]\n",
    "}\n",
    "\n",
    "rfr_grid = GridSearchCV(rfr, rfr_param_grid, cv=5, scoring='neg_mean_squared_error' ,n_jobs=-1)\n",
    "rfr_grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(-1 * rfr_grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr_grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGboost Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBRegressor()\n",
    "xgb_param_grid = {\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5],\n",
    "    'n_estimators': [100, 250, 500]\n",
    "}\n",
    "\n",
    "xgb_grid = GridSearchCV(\n",
    "    xgb, \n",
    "    xgb_param_grid, \n",
    "    cv=5, \n",
    "    scoring='neg_mean_squared_error', \n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "xgb_grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(-1 * xgb_grid.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = Ridge()\n",
    "ridge_param_grid = {\n",
    "    'alpha': [0.05, 0.1, 1, 3, 5],\n",
    "    'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga']\n",
    "}\n",
    "\n",
    "ridge_grid = GridSearchCV(\n",
    "    ridge, \n",
    "    ridge_param_grid, \n",
    "    cv=5, \n",
    "    scoring='neg_mean_squared_error', \n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "ridge_grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(-1 * ridge_grid.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdr = GradientBoostingRegressor()\n",
    "\n",
    "gdr_param_grid = {\n",
    "    'learning_rate': [0.01, 0.1, 0.001],\n",
    "    'max_depth': [10,15,20],\n",
    "    'n_estimators': [100, 250, 500, 1000],\n",
    "    'min_samples_split': [10, 25, 50], \n",
    "    'max_features': [0.01, 0.1, 0.5]\n",
    "}\n",
    "\n",
    "gdr_grid = GridSearchCV(\n",
    "    gdr, \n",
    "    gdr_param_grid, \n",
    "    cv=5, \n",
    "    scoring='neg_mean_squared_error', \n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "gdr_grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vr = VotingRegressor(\n",
    "    estimators=[\n",
    "        ('rfr', rfr_grid.best_estimator_), \n",
    "        ('xgb', xgb_grid.best_estimator_), \n",
    "        ('ridge', ridge_grid.best_estimator_), \n",
    "        ('gdr', gdr_grid.best_estimator_)\n",
    "    ]\n",
    ")\n",
    "\n",
    "vr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_vr = vr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"__Erreur quadratique moyenne__\")\n",
    "print(mean_squared_error(y_test, y_pred_vr))\n",
    "print(\"__Coefficient de détermination__\")\n",
    "print(r2_score(y_test, y_pred_vr))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projFinalVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
